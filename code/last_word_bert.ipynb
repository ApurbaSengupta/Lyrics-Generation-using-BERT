{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "last-word-bert.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYHRcp62BLqS",
        "colab_type": "code",
        "outputId": "15cc9453-2480-40d2-ffa4-43f847ad28c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.1.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.139)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.139 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.139)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2_KKFQqZ_3d",
        "colab_type": "code",
        "outputId": "e6f71539-14ba-49ac-924f-734c0ec8d09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!pip install pronouncing"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0 (from pronouncing)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/bc/606843d7cfe4d82f5a21fc46d1ae8e364ac20c57e68d1ec4190bce4f2734/cmudict-0.4.2-py2.py3-none-any.whl (938kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 5.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.2 pronouncing-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0KZMlB1JnpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction\n",
        "import pronouncing\n",
        "from itertools import chain\n",
        "import string\n",
        "import csv\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbVGLH4MMsPY",
        "colab_type": "code",
        "outputId": "4aeec742-3600-4f15-c699-901e28a6b470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# mount Google Drive root\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc6gw-GC1RK-",
        "colab_type": "code",
        "outputId": "0daf2cbc-7ead-4ef2-c44e-e894d6d50ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        }
      },
      "source": [
        "\"\"\" Try to generate from BERT \"\"\"\n",
        "\n",
        "def preprocess(tokens, tokenizer, device):\n",
        "    \"\"\" Preprocess the lyrics by tokenizing and converting to tensor \"\"\"\n",
        "    \n",
        "    tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tok_tensor = torch.tensor([tok_ids])\n",
        "    tok_tensor = tok_tensor.to(device)\n",
        "    return tok_tensor\n",
        "\n",
        "  \n",
        "def get_seed_sent(toks1, toks2, tokenizer):\n",
        "    \"\"\" Get initial sentence to decode from, possible with masks \"\"\"\n",
        "\n",
        "    mask_ids = []\n",
        "\n",
        "    # get total lyric tokens and [MASK] indices\n",
        "    toks = toks1 + toks2\n",
        "    for i, tok in enumerate(toks):\n",
        "        if tok == \"[MASK]\":\n",
        "            mask_ids.append(i)\n",
        "            \n",
        "    # get lyric segments\n",
        "    seg = [0] * len(toks1) + [1] * len(toks2)\n",
        "    \n",
        "    # convert segments to tensors\n",
        "    seg_tensor = torch.tensor([seg])\n",
        "    \n",
        "    return toks, seg_tensor, mask_ids\n",
        "\n",
        "  \n",
        "def load_masked_lang_model(version):\n",
        "    \"\"\" Load BERT MLM \"\"\"\n",
        "    model = BertForMaskedLM.from_pretrained(version)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_next_sent_pred_model(version):\n",
        "    \"\"\" Load BERT next sentence prediction model \"\"\"\n",
        "    model = BertForNextSentencePrediction.from_pretrained(version)\n",
        "    model.eval()\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def predict(model, tokenizer, tok_tensor, seg_tensor, how_select=\"argmax\"):\n",
        "    \"\"\" Get model predictions and convert back to tokens \"\"\"\n",
        "    preds = model(tok_tensor, seg_tensor)\n",
        "    \n",
        "    # select random if \"sample\"\n",
        "    if how_select == \"sample\":\n",
        "        dist = Categorical(logits=F.log_softmax(preds[0], dim=-1))\n",
        "        pred_idxs = dist.sample().tolist()\n",
        "        \n",
        "    # select top-3 if \"topk\"\n",
        "    elif how_select == \"topk\":\n",
        "        kth_vals, kth_idx = F.log_softmax(preds[0], dim=-1).topk(3, dim=-1)\n",
        "        dist = Categorical(logits=kth_vals)\n",
        "        pred_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
        "        \n",
        "    # select best possible if \"argmax\"\n",
        "    elif how_select == \"argmax\":\n",
        "        pred_idxs = preds.argmax(dim=-1).tolist()[0]\n",
        "        \n",
        "    # if none of the above, raise error\n",
        "    else:\n",
        "        raise NotImplementedError(\"Prediction procedure %s not found!\" % how_select)\n",
        "    \n",
        "    # return predicted [MASK] tags\n",
        "    pred_toks = tokenizer.convert_ids_to_tokens(pred_idxs)\n",
        "    return pred_toks\n",
        "\n",
        "  \n",
        "def masked_decoding(toks, device, seg_tensor, masks, model, tokenizer, selection_strategy):\n",
        "    \"\"\" Decode from model by replacing masks \"\"\"\n",
        "    for step_n, mask_id in enumerate(masks):\n",
        "        tok_tensor = preprocess(toks, tokenizer, device)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[mask_id] = pred_toks[mask_id]\n",
        "    return toks\n",
        "\n",
        "  \n",
        "def best_follows(text1_tokens, text2_tokens, tokenizer, model, device, k=1):\n",
        "    \"\"\" Return k best next sentence predictions \"\"\"\n",
        "    \n",
        "    # get seed lyric tokens and segment and attention lists\n",
        "    text1_token_ids = tokenizer.convert_tokens_to_ids(text1_tokens)\n",
        "    text1_seg = [0] * len(text1_token_ids)\n",
        "    text1_attention = [1] * len(text1_token_ids)\n",
        "    \n",
        "    tok_ids = []\n",
        "    tok_segs = []\n",
        "    tok_attentions = []\n",
        "    \n",
        "    # get target lyric tokens and segment and attention lists\n",
        "    text2_token_ids = []\n",
        "    for text2_token in text2_tokens:\n",
        "        text2_token_ids.append(tokenizer.convert_tokens_to_ids(text2_token))\n",
        "    \n",
        "    max_text2_length = max(len(text2_token_id) for text2_token_id in text2_token_ids)\n",
        "    \n",
        "    # get total lyric tokens and segment and attention lists\n",
        "    for text2_token_id in text2_token_ids:\n",
        "        padding_size = max_text2_length - len(text2_token_id)\n",
        "        padded_text2_id = text2_token_id + [0] * padding_size\n",
        "        padded_text2_seg = [1] * max_text2_length\n",
        "        padded_text2_attention = [1] * len(text2_token_id) + [0] * padding_size\n",
        "        \n",
        "        tok_ids.append(text1_token_ids + padded_text2_id)\n",
        "        tok_segs.append(text1_seg + padded_text2_seg)\n",
        "        tok_attentions.append(text1_attention + padded_text2_attention)\n",
        "    \n",
        "    # convert tokens and segment and attention lists to tensors\n",
        "    tok_ids_tensor = torch.LongTensor(tok_ids)\n",
        "    tok_segs_tensor = torch.LongTensor(tok_segs)\n",
        "    tok_attention_tensor = torch.LongTensor(tok_attentions)\n",
        "    \n",
        "    # transport to device\n",
        "    tok_ids_tensor = tok_ids_tensor.to(device)\n",
        "    tok_segs_tensor = tok_segs_tensor.to(device)\n",
        "    tok_attention_tensor = tok_attention_tensor.to(device)\n",
        "\n",
        "    # get is next or not next predictions\n",
        "    seq_relationship_logits = model(tok_ids_tensor, tok_segs_tensor, tok_attention_tensor)\n",
        "    \n",
        "    # get the top-3/top-1 predictions and return the respective tokens\n",
        "    _, idxs = torch.topk(seq_relationship_logits[:,0], k)\n",
        "    \n",
        "    return [text2_tokens[i] for i in idxs.tolist()]\n",
        "\n",
        " \n",
        "\n",
        "def totalTokens(five):\n",
        "    tokens = 0\n",
        "    for line in five:\n",
        "      tokens += len(line.split())\n",
        "      if(tokens > 500):\n",
        "        return False\n",
        "      \n",
        "    return True\n",
        "                \n",
        "@torch.no_grad()  \n",
        "def main():\n",
        "    \n",
        "    # set device : use CUDA backend if GPU available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # get BERT tokenizer and pre-trained models\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    masked_lang_model = load_masked_lang_model('bert-base-uncased')\n",
        "    next_sent_pred_model = load_next_sent_pred_model('bert-base-uncased')\n",
        "    \n",
        "    # transport model to device\n",
        "    masked_lang_model = masked_lang_model.to(device)\n",
        "    next_sent_pred_model = next_sent_pred_model.to(device)\n",
        "    \n",
        "    # get data\n",
        "    five_pairs = []\n",
        "    with open(\"gdrive/My Drive/courseWorks/Lyrics/data/test_rock.csv\", encoding='utf8') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            lines = row['lyrics'].split('\\n')\n",
        "            for i in range(0,len(lines)-4,5):\n",
        "                current_five = [lines[i].strip(), lines[i + 1].strip(), lines[i + 2].strip(), lines[i + 3].strip(), lines[i + 4].strip()]\n",
        "                \n",
        "                five_pairs.append(current_five)\n",
        "    cnt = 0\n",
        "    \n",
        "    # predictions for each pair of 5 lyrics\n",
        "    for five_list in five_pairs:\n",
        "        \n",
        "        \"\"\" Pre-process texts to get seed text and target texts \"\"\"\n",
        "        \n",
        "        text = (\"\\n\").join(five_list)\n",
        "        \n",
        "        # put [CLS] and [SEP] tags for lyric 'beginning' and 'end' annotation\n",
        "        text = text.replace(\"\\n\",\" [SEP] \")\n",
        "        text = \"[CLS] \" + text\n",
        "        \n",
        "        cont = False\n",
        "        # add/keep punctuation at end of every other lyric\n",
        "        text_split = text.split(\" [SEP] \")\n",
        "        for i in range(len(text_split)):\n",
        "            if i%2 != 0:\n",
        "              try:\n",
        "                if text_split[i][-1] not in string.punctuation:\n",
        "                    text_split[i] = text_split[i] + \".\"\n",
        "              except IndexError:\n",
        "                cont = True\n",
        "#                 pdb.set_trace()\n",
        "        \n",
        "        if cont:\n",
        "          continue\n",
        "        # seed length\n",
        "        text1_len = 3\n",
        "        \n",
        "        # get seed text tokens\n",
        "        text1 = (\" [SEP] \").join(text_split[:text1_len]) + \" [SEP]\"\n",
        "        toks1 = tokenizer.tokenize(text1)\n",
        "\n",
        "        text2s = text_split[text1_len:]\n",
        "        text2s_copy = text2s.copy()\n",
        "        \n",
        "        # mask desired target lyric with [MASK] and [RHYME] tags\n",
        "        for i in range(len(text2s)):\n",
        "            if i%2 == 0:\n",
        "                text2s[i] = ' '.join(text2s[i].split()[:len(text2s[i].split())-1]) + \" [MASK] \" + \". [SEP] \"\n",
        "                if text2s_copy[i][-1] not in string.punctuation:\n",
        "                  text2s_copy[i] = text2s_copy[i] + \". [SEP] \"\n",
        "                else:\n",
        "                  text2s_copy[i] = text2s_copy[i] + \" [SEP] \"\n",
        "            else:\n",
        "                text2s[i] = text2s[i] + \" [SEP] \"\n",
        "                text2s_copy[i] = text2s_copy[i] + \" [SEP] \"\n",
        "\n",
        "        text2 = (\" \").join(text2s)\n",
        "        text2_copy = (\" \").join(text2s_copy)\n",
        "        toks2_copy = tokenizer.tokenize(text2_copy)\n",
        "        \n",
        "\n",
        "        \n",
        "        outputs = []\n",
        "        \n",
        "            \n",
        "        # get target lyric tokens\n",
        "        toks2 = tokenizer.tokenize(text2)\n",
        "\n",
        "        # get lyric tokens, segment tensors and mask indices\n",
        "        toks, seg_tensor, mask_ids = get_seed_sent(toks1, toks2, tokenizer)\n",
        "#             print(\"toks # \", len(toks))\n",
        "        if(len(toks) > 512):\n",
        "#           print(\"toks # \", len(toks))\n",
        "          continue\n",
        "        seg_tensor = seg_tensor.to(device)\n",
        "\n",
        "        # get [MASK] predictions\n",
        "        pred_toks = masked_decoding(toks, device, seg_tensor, mask_ids, masked_lang_model, tokenizer, \"argmax\")\n",
        "\n",
        "        # get predicted lyric\n",
        "        outputs.append(pred_toks[len(toks1):])\n",
        "        \n",
        "#         get top-3/top-1 best sequence as predicted by the BERT next sentence prediction model\n",
        "        \n",
        "        outz = best_follows(toks1, outputs, tokenizer, next_sent_pred_model, device, k=1)\n",
        "\n",
        "#         with open(\"gdrive/My Drive/courseWorks/Lyrics/preds/val_pred_file.txt\", \"a\") as file:\n",
        "#             file.write(\"\\n Top-\"+str(len(outz))+\" predictions: \"+str(outz)+\"\\n\")\n",
        "        \n",
        "        # print the best possible lyric\n",
        "        toks_pred = toks1 + outz[0]\n",
        "        \n",
        "        with open(\"gdrive/My Drive/courseWorks/Lyrics/preds/test_pred_file.txt\", \"a\") as file:\n",
        "            file.write(\"\\n\"+(\" \").join(toks_pred)+\"\\n\")\n",
        "            \n",
        "        toks_gold = toks1 + toks2_copy\n",
        "        with open(\"gdrive/My Drive/courseWorks/Lyrics/preds/test_gold_file.txt\", \"a\") as file:\n",
        "            file.write(\"\\n\"+(\" \").join(toks_gold) +\"\\n\")\n",
        "        \n",
        "        # clear cache\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        cnt += 1\n",
        "        \n",
        "        if cnt%1000 == 0:\n",
        "            \n",
        "            print(\"Processed %s 5-pairs\" %cnt)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 2516841.14B/s]\n",
            "100%|██████████| 407873900/407873900 [00:07<00:00, 58231277.84B/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed 1000 5-pairs\n",
            "Processed 2000 5-pairs\n",
            "Processed 3000 5-pairs\n",
            "Processed 4000 5-pairs\n",
            "Processed 5000 5-pairs\n",
            "Processed 6000 5-pairs\n",
            "Processed 7000 5-pairs\n",
            "Processed 8000 5-pairs\n",
            "Processed 9000 5-pairs\n",
            "Processed 10000 5-pairs\n",
            "Processed 11000 5-pairs\n",
            "Processed 12000 5-pairs\n",
            "Processed 13000 5-pairs\n",
            "Processed 14000 5-pairs\n",
            "Processed 15000 5-pairs\n",
            "Processed 16000 5-pairs\n",
            "Processed 17000 5-pairs\n",
            "Processed 18000 5-pairs\n",
            "Processed 19000 5-pairs\n",
            "Processed 20000 5-pairs\n",
            "Processed 21000 5-pairs\n",
            "Processed 22000 5-pairs\n",
            "Processed 23000 5-pairs\n",
            "Processed 24000 5-pairs\n",
            "Processed 25000 5-pairs\n",
            "Processed 26000 5-pairs\n",
            "Processed 27000 5-pairs\n",
            "Processed 28000 5-pairs\n",
            "Processed 29000 5-pairs\n",
            "Processed 30000 5-pairs\n",
            "Processed 31000 5-pairs\n",
            "Processed 32000 5-pairs\n",
            "Processed 33000 5-pairs\n",
            "Processed 34000 5-pairs\n",
            "Processed 35000 5-pairs\n",
            "Processed 36000 5-pairs\n",
            "Processed 37000 5-pairs\n",
            "Processed 38000 5-pairs\n",
            "Processed 39000 5-pairs\n",
            "Processed 40000 5-pairs\n",
            "Processed 41000 5-pairs\n",
            "Processed 42000 5-pairs\n",
            "Processed 43000 5-pairs\n",
            "Processed 44000 5-pairs\n",
            "Processed 45000 5-pairs\n",
            "Processed 46000 5-pairs\n",
            "Processed 47000 5-pairs\n",
            "Processed 48000 5-pairs\n",
            "Processed 49000 5-pairs\n",
            "Processed 50000 5-pairs\n",
            "Processed 51000 5-pairs\n",
            "Processed 52000 5-pairs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arY_9H2IlXSu",
        "colab_type": "code",
        "outputId": "000f80be-4238-4a04-835e-21667c9ba951",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def loadData(name):\n",
        "    data = []\n",
        "    with open(name) as file:\n",
        "        for line in file:\n",
        "            if(line == '\\n'):\n",
        "              continue\n",
        "#             pdb.set_trace()\n",
        "            data.append(line.split(\" [SEP] \")[-2])\n",
        "\n",
        "    return data\n",
        "\n",
        "def bleuScore(gold, pred):\n",
        "    cumulativeBlue, totalSentences = 0, len(gold)\n",
        "\n",
        "    for i in range(len(gold)):\n",
        "       \n",
        "        reference = [gold[i].split(' ')]\n",
        "        candidate = pred[i].split(' ') \n",
        "        cumulativeBlue += sentence_bleu(reference, candidate, weights=(.334, 0.333, 0.333, 0))\n",
        "\n",
        "    return cumulativeBlue / totalSentences  \n",
        "\n",
        "def accuracy(gold, pred):\n",
        "    num_correct, num_total = 0, 0\n",
        "    for i in range(len(gold)):\n",
        "        if gold[i] == pred[i]:\n",
        "            num_correct += 1\n",
        "        num_total += 1\n",
        "\n",
        "    accuracy = num_correct / num_total\n",
        "\n",
        "    return accuracy\n",
        "  \n",
        "\n",
        "if __name__ == '__main__':\n",
        "  gold = loadData(\"gdrive/My Drive/courseWorks/Lyrics/preds/test_gold_file.txt\")\n",
        "  pred = loadData(\"gdrive/My Drive/courseWorks/Lyrics/preds/test_pred_file.txt\")\n",
        "  print(f'Accuracy: {accuracy(gold, pred):.2f}')\n",
        "  print(f'BLEU score: {bleuScore(gold, pred):.2f}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.31\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}