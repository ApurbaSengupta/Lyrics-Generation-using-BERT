{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-baselines.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGoGtt4XK1wJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35f77bf4-7e4a-4e27-9eb4-fa4917e2a0dc"
      },
      "source": [
        "# mount Google Drive root\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYOBu1QqLDyV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "678ad1eb-6ea2-4e92-e8fc-0c0d5cf5c6de"
      },
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.16.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2018.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.9.139)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.2.0)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.139 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.12.139)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.139->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPWcx_-FLFL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForMaskedLM, BertForNextSentencePrediction\n",
        "import csv\n",
        "import random\n",
        "import argparse\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import warnings\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk9hg_OLnPD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def predict_masked_word(tokenizer, model, device):\n",
        "  \n",
        "    # Tokenized input\n",
        "    text1 = \"[CLS] And the riot squad they're restless, they need somewhere to go [SEP]\"\n",
        "    text2 = \"As Lady and I look out tonight, from Desolation Row. [SEP]\"\n",
        "    tokenized_text1 = tokenizer.tokenize(text1)\n",
        "    tokenized_text2 = tokenizer.tokenize(text2)\n",
        "    tokenized_text = tokenized_text1 + tokenized_text2\n",
        "\n",
        "    # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
        "    masked_index = tokenized_text2.index('lady') + len(tokenized_text1)\n",
        "    tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "    # Convert token to vocabulary indices\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
        "    segments_ids = [0]*len(tokenized_text1) + [1]*(len(tokenized_text2))\n",
        "\n",
        "    # Convert inputs to PyTorch tensors\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    tokens_tensor = tokens_tensor.to(device)\n",
        "    segments_tensors = segments_tensors.to(device)\n",
        "\n",
        "    predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "    # Confirm we were able to predict the correct '[MASK]'\n",
        "    predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "    print(\"\\n\",\"Predicted [MASK] = \",predicted_token,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEB97pexLXJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_sentence(sentenceA, sentenceBs, tokenizer, model, device):\n",
        "    \n",
        "    sentenceA_toks = tokenizer.tokenize(sentenceA)\n",
        "    sentenceA_ids = tokenizer.convert_tokens_to_ids(sentenceA_toks)\n",
        "    sentenceA_types = [0] * len(sentenceA_ids)\n",
        "    sentenceA_attention = [1] * len(sentenceA_ids)\n",
        "    tok_ids = []\n",
        "    tok_types = []\n",
        "    tok_attention = []\n",
        "    \n",
        "    sentenceBs_ids = []\n",
        "    for sentenceB in sentenceBs:\n",
        "        sentenceB_toks = tokenizer.tokenize(sentenceB)\n",
        "        sentenceB_ids = tokenizer.convert_tokens_to_ids(sentenceB_toks)\n",
        "        sentenceBs_ids.append(sentenceB_ids)\n",
        "        \n",
        "    max_sentenceB_length = max(len(sentenceB_ids) for sentenceB_ids in sentenceBs_ids)\n",
        "    for sentenceB_ids in sentenceBs_ids:\n",
        "        padding_size = max_sentenceB_length - len(sentenceB_ids)\n",
        "        padded_sentenceB_ids = sentenceB_ids + [0] * padding_size\n",
        "        padded_sentenceB_types = [1] * max_sentenceB_length\n",
        "        padded_sentenceB_attention = [1] * len(sentenceB_ids) + [0] * padding_size\n",
        "        tok_ids.append(sentenceA_ids + padded_sentenceB_ids)\n",
        "        tok_types.append(sentenceA_types + padded_sentenceB_types)\n",
        "        tok_attention.append(sentenceA_attention + padded_sentenceB_attention)\n",
        "    \n",
        "    tok_ids_tensor = torch.LongTensor(tok_ids)\n",
        "    tok_types_tensor = torch.LongTensor(tok_types)\n",
        "    tok_attention_tensor = torch.LongTensor(tok_attention)\n",
        "    \n",
        "    tok_ids_tensor = tok_ids_tensor.to(device)\n",
        "    tok_types_tensor = tok_types_tensor.to(device)\n",
        "    tok_attention_tensor = tok_attention_tensor.to(device)\n",
        "    \n",
        "    seq_relationship_logits = model(tok_ids_tensor, tok_types_tensor, tok_attention_tensor)\n",
        "    \n",
        "    return sentenceBs[seq_relationship_logits[:,0].argmax().tolist()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlavyt6VNZ-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@torch.no_grad()\n",
        "def generate_predictions(args):\n",
        "    all_lines = []\n",
        "    all_pairs = []\n",
        "    with open(args.datafile, encoding='utf8') as csv_file:\n",
        "        csv_reader = csv.DictReader(csv_file)\n",
        "        for row in csv_reader:\n",
        "            lines = row['lyrics'].split('\\n')\n",
        "            for i in range(len(lines) - 1):\n",
        "                all_pairs.append((lines[i], lines[i + 1]))\n",
        "                all_lines.append(lines[i])\n",
        "            all_lines.append(lines[len(lines) - 1])\n",
        "\n",
        "    sampled_data_x = {}\n",
        "    sampled_data_y = {}\n",
        "    correct_pairs = random.sample(all_pairs, 100)\n",
        "    for line1, line2 in correct_pairs:\n",
        "        sampled_data_y[line1] = line2\n",
        "        sampled_data_x[line1] = [line2]\n",
        "        sampled_data_x[line1].extend(random.sample(all_lines, 2))\n",
        "\n",
        "    with open('gdrive/My Drive/CIS530/predfile', 'w') as file:\n",
        "        for i, (line1, line2s) in enumerate(sampled_data_x.items()):\n",
        "            line2 = predict_next_sentence(line1, line2s, tokenizer, next_sent_pred_model, device)\n",
        "            file.write(f'{line1}\\t{line2}\\n')\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f'Finished predicting {i + 1} lines...')\n",
        "    with open('gdrive/My Drive/CIS530/goldfile', 'w') as file:\n",
        "        for line1, line2 in sampled_data_y.items():\n",
        "            file.write(f'{line1}\\t{line2}\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYD-t8uGNtEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "masked_model.eval()\n",
        "next_sent_pred_model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "next_sent_pred_model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "masked_model = masked_model.to(device)\n",
        "next_sent_pred_model = next_sent_pred_model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z61OUT4o_4G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "770eca88-b96f-4501-aadf-38efe4903018"
      },
      "source": [
        "predict_masked_word(tokenizer, masked_model, device)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Predicted [MASK] =  dad \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TKvRPVArHxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "96830a2b-660c-49d3-d509-73fee14f6549"
      },
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--datafile', type=str, required=True)\n",
        "args = parser.parse_args(['--datafile', 'gdrive/My Drive/CIS530/Lyrics-Generation/data/test_rock.csv'])\n",
        "generate_predictions(args)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished predicting 10 lines...\n",
            "Finished predicting 20 lines...\n",
            "Finished predicting 30 lines...\n",
            "Finished predicting 40 lines...\n",
            "Finished predicting 50 lines...\n",
            "Finished predicting 60 lines...\n",
            "Finished predicting 70 lines...\n",
            "Finished predicting 80 lines...\n",
            "Finished predicting 90 lines...\n",
            "Finished predicting 100 lines...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY8o6qMcPk_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0b362b04-31a6-4bca-a3c4-6fb71b7a6351"
      },
      "source": [
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "def bleuScore(gold, pred):\n",
        "    cumulativeBlue, totalSentences = 0, len(gold)\n",
        "\n",
        "    for line in gold:\n",
        "        assert line in pred\n",
        "        reference = [gold[line].split(' ')]\n",
        "        candidate = pred[line].split(' ') \n",
        "        cumulativeBlue += sentence_bleu(reference, candidate, weights=(.334, 0.333, 0.333, 0))\n",
        "\n",
        "    return cumulativeBlue / totalSentences\n",
        "\n",
        "def accuracy(gold, pred):\n",
        "    num_correct, num_total = 0, 0\n",
        "    for line1 in gold:\n",
        "        assert line1 in pred\n",
        "        if gold[line1] == pred[line1]:\n",
        "            num_correct += 1\n",
        "        num_total += 1\n",
        "\n",
        "    accuracy = num_correct / num_total\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def loadData(name):\n",
        "    data = {}\n",
        "    with open(name) as file:\n",
        "        for line in file:\n",
        "            line1, line2 = line.strip().split('\\t')\n",
        "            data[line1] = line2\n",
        "\n",
        "    return data\n",
        "\n",
        "def main(args):\n",
        "    gold = loadData(args.goldfile)\n",
        "    pred = loadData(args.predfile)\n",
        "\n",
        "    assert len(gold) == len(pred)\n",
        "\n",
        "    print(f'Accuracy: {accuracy(gold, pred):.2f}')\n",
        "    print(f'BLEU score: {bleuScore(gold, pred):.2f}')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--goldfile', type=str, required=True)\n",
        "    parser.add_argument('--predfile', type=str, required=True)\n",
        "\n",
        "    args = parser.parse_args(['--goldfile', 'gdrive/My Drive/CIS530/goldfile', '--predfile', 'gdrive/My Drive/CIS530/predfile'])\n",
        "    main(args)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.47\n",
            "BLEU score: 0.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxVlrCzysDxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "65255f2b-dee8-429b-884a-f62d0de1e349"
      },
      "source": [
        "\"\"\" Try to generate from BERT \"\"\"\n",
        "\n",
        "MASK = \"[MASK]\"\n",
        "MASK_ATOM = \"[MASK]\"\n",
        "\n",
        "def preprocess(tokens, tokenizer, device):\n",
        "    \"\"\" Preprocess the sentence by tokenizing and converting to tensor \"\"\"\n",
        "    tok_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    tok_tensor = torch.tensor([tok_ids])\n",
        "    tok_tensor = tok_tensor.to(device)\n",
        "    return tok_tensor\n",
        "\n",
        "\n",
        "def get_mask_ids(masking):\n",
        "    if masking:\n",
        "      mask_ids = [int(d) for d in masking.split(',')]\n",
        "    else:\n",
        "      mask_ids = []     \n",
        "    return mask_ids\n",
        "\n",
        "  \n",
        "def get_seed_sent(seed_sentence, tokenizer, masking=None, n_append_mask=0):\n",
        "    \"\"\" Get initial sentence to decode from, possible with masks \"\"\"\n",
        "\n",
        "    # Get initial mask\n",
        "    mask_ids = get_mask_ids(masking)\n",
        "\n",
        "    # Tokenize, respecting [MASK]\n",
        "    seed_sentence = seed_sentence.replace(MASK, MASK_ATOM)\n",
        "    toks = tokenizer.tokenize(seed_sentence)\n",
        "    for i, tok in enumerate(toks):\n",
        "        if tok == MASK_ATOM:\n",
        "            mask_ids.append(i)\n",
        "\n",
        "    # Mask the input\n",
        "    for mask_id in mask_ids:\n",
        "        toks[mask_id] = MASK\n",
        "\n",
        "    # Append MASKs\n",
        "    for _ in range(n_append_mask):\n",
        "        mask_ids.append(len(toks))\n",
        "        toks.append(MASK)\n",
        "    mask_ids = sorted(list(set(mask_ids)))\n",
        "\n",
        "    seg = [0] * len(toks)\n",
        "    seg_tensor = torch.tensor([seg])\n",
        "    return toks, seg_tensor, mask_ids\n",
        "\n",
        "  \n",
        "def load_model(version):\n",
        "    \"\"\" Load model \"\"\"\n",
        "    model = BertForMaskedLM.from_pretrained(version)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(model, tokenizer, tok_tensor, seg_tensor, how_select=\"argmax\"):\n",
        "    \"\"\" Get model predictions and convert back to tokens \"\"\"\n",
        "    preds = model(tok_tensor, seg_tensor)\n",
        "\n",
        "    if how_select == \"sample\":\n",
        "        dist = Categorical(logits=F.log_softmax(preds[0], dim=-1))\n",
        "        pred_idxs = dist.sample().tolist()\n",
        "    elif how_select == \"topk\":\n",
        "        kth_vals, kth_idx = F.log_softmax(preds[0], dim=-1).topk(3, dim=-1)\n",
        "        dist = Categorical(logits=kth_vals)\n",
        "        pred_idxs = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1).tolist()\n",
        "    elif how_select == \"argmax\":\n",
        "        pred_idxs = preds.argmax(dim=-1).tolist()[0]\n",
        "    else:\n",
        "        raise NotImplementedError(\"Prediction procedure %s not found!\" % how_select)\n",
        "\n",
        "    pred_toks = tokenizer.convert_ids_to_tokens(pred_idxs)\n",
        "    return pred_toks\n",
        "  \n",
        "\n",
        "def masked_decoding(toks, seg_tensor, masks, model, tokenizer, device, selection_strategy):\n",
        "    \"\"\" Decode from model by replacing masks \"\"\"\n",
        "    for step_n, mask_id in enumerate(masks):\n",
        "        tok_tensor = preprocess(toks, tokenizer, device)\n",
        "        pred_toks = predict(model, tokenizer, tok_tensor, seg_tensor, selection_strategy)\n",
        "        toks[mask_id] = pred_toks[mask_id]\n",
        "    return toks\n",
        "\n",
        "  \n",
        "def detokenize(pred_toks):\n",
        "    \"\"\" Return the detokenized lyric prediction \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(pred_toks):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def main():\n",
        "\n",
        "    toks, seg_tensor, mask_ids = get_seed_sent(\"[CLS] Sing with me, Sing for the years [SEP] [MASK] [MASK] [MASK] [MASK] , [MASK] [MASK] [MASK] tears. [SEP]\",\n",
        "                                               tokenizer,\n",
        "                                               masking=None,\n",
        "                                               n_append_mask=0)\n",
        "    \n",
        "    seg_tensor = seg_tensor.to(device)\n",
        "        \n",
        "    pred_toks = masked_decoding(toks, seg_tensor, mask_ids, masked_model, tokenizer, device, \"argmax\")\n",
        "    \n",
        "    pred_lyric = detokenize(pred_toks)\n",
        "    \n",
        "    print(\"\\nFinal: %s\" % (\" \".join(pred_lyric)),\"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Final: [CLS] sing with me , sing for the years [SEP] sing with me now , sing for the tears . [SEP] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}